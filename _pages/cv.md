---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

This is a short version of my resume. For a full list of publications, check out my [Google Scholar page](https://scholar.google.com/citations?user=uCJk8mIAAAAJ&hl=en). 

Education
======
* Ph.D in Computer Science, University of Washington, 2022
  * Thesis: How to train your self-supervised NLP model: Investigating pre-training objectives, data, and scale.
  * Advisers: [Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/lsz) and [Dan Weld](https://www.cs.washington.edu/people/faculty/weld)
* Master of Technology in Computer Science, IIT Bombay, 2014
* Bachelor of Technology in Computer Science, VNIT Nagpur, 2012

Employment
======
* Research Scientist, Google Brain (May 2022 - Present)
* Visiting Researcher, Meta AI Research (formerly FAIR) (April 2020 - March 2022})
* Software Engineer, IBM Research (August 2014 - May 2015)
* Internships: Google (Summer 2019), AI2 (Fall 2017)
  
Representative Publications
======
Kenton Lee\*, <b>Mandar Joshi</b>\*, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova. <a href="https://arxiv.org/abs/2210.03347" target="_blank"> Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding</a>. ArXiv 2210.03347, 2022.<br />
\* equal contribution

Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, <b>Mandar Joshi</b>, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer. <a href="https://arxiv.org/abs/2201.07520" target="_blank"> CM3: A Causal Masked Multimodal Model of the Internet </a>. ArXiv 2201.07520, 2022.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, <b>Mandar Joshi</b>, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. <a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach </a>. ArXiv:1907.11692, 2019.  [<a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank"> code </a>]

<b>Mandar Joshi</b>*, Danqi Chen*, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy. <a href="https://arxiv.org/abs/1907.10529" target="_blank"> SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>. TACL, 2019.  Equal Contribution  [<a href="https://github.com/facebookresearch/SpanBERT" target="_blank"> code </a>]
<br />
\* equal contribution

<b>Mandar Joshi</b>, Eunsol Choi, Daniel Weld, Luke Zettlemoyer. <a href = "docs/triviaQA.pdf" target = "_blank">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a>. Association for Computational Linguistics (ACL) 2017. [ <a href="http://nlp.cs.washington.edu/triviaqa/" target="_blank">website</a> ] [<a href="bibs/triviaqa.bib" target="_blank"> bib </a>]
  
Misc
======
* Invited talk on “Efficient Scalable Pre-training for Natural Language Processing” at KDD 2020 Deep
Learning Day.
* Awarded Microsoft Endowment Fellowship for the academic year 2015-2016.
* All India Rank 2 in GATE (Graduate Aptitude Test in Engineering) 2012 amongst 150,000 applicants. The test is conducted by IITs for admission into their graduate programs.
Service and Courses.